# -- coding: utf-8 --
"""22_IT_EXP_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_pE4PGvsi-mN7nWUynB8Payv3GgdgHxP

Sentiment analysis - Amazon Health and Personal Care Reviews
"""

!git clone https://gitlab.com/sayantan.world98/sentiment-analysis-amazon-health-and-personal-care.git

import pandas as pd

import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
import re
import string
import pickle

df = pd.read_json('Health_and_Personal_Care_5.json', lines=True)

df.head()

df['overall'].value_counts()

positives = df['overall'][(df.overall == 5) | (df.overall == 4)]
negatives = df['overall'][(df.overall == 2) | (df.overall == 1)]
print('Total length of the data is:         {}'.format(df.shape[0]))
print('No. of positve tagged sentences is:  {}'.format(len(positives)))
print('No. of negative tagged sentences is: {}'.format(len(negatives)))

df_pos = df[df['overall'].isin([4, 5])]
df_neg = df[df['overall'].isin([1, 2])]

df['score'] = df.helpful.apply(lambda x: x[0] / (x[1] + 2))

df_all = pd.concat([df_pos, df_neg])

"""Perform Binary Classification"""

def sentiment_score(n):
  if n == 5 or n == 4:
    return 1
  return 0

df_all['overall']=df_all['overall'].apply(lambda x: sentiment_score(x))

df_all['overall'].value_counts()

df_all.head()

df_all = df_all.drop(['reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', 'summary', 'asin', 'reviewerID'], axis=1, errors = "ignore")

df_all.head()

df_all.reset_index().drop(['index'], axis=1)

"""Data Visualization"""

def word_count(words):
    return len(words.split())

df_all['word count'] = df_all['reviewText'].apply(word_count)

p = df_all['word count'][df_all.overall == 1]
n = df_all['word count'][df_all.overall == 0]
plt.figure(figsize=(12,6))
plt.xlim(0,400)
plt.xlabel('Word count')
plt.ylabel('Frequency')
g = plt.hist([p, n], color=['g','r'], alpha=0.5, label=['positive','negative'], bins=50)
plt.legend(loc='upper right')

df_all = df_all.drop(['word count'], axis=1)

"""Data Processing"""

nltk.download('stopwords')
stopword = set(stopwords.words('english'))

wordLemm = WordNetLemmatizer()

def process_reviews(review):
    review = review.lower()
    review = review.translate(str.maketrans("","",string.punctuation))
    tokens = word_tokenize(review)
    final_tokens = [w for w in tokens if w not in stopword]
    finalwords=[]
    for w in final_tokens:
      if len(w)>1:
        word = wordLemm.lemmatize(w)
        finalwords.append(word)
    return ' '.join(finalwords)

df_all['reviewText'] = df_all['reviewText'].astype('str')

nltk.download('punkt')
nltk.download('wordnet')

import nltk

nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('stopwords', quiet=True)

try:
    nltk.download('punkt_tab', quiet=True)
except LookupError:
    print("Warning: 'punkt_tab' resource not found or could not be downloaded.")

wordLemm = WordNetLemmatizer()
stopword = set(stopwords.words('english'))

def process_reviews(review):
    review = review.lower()
    review = review.translate(str.maketrans("","",string.punctuation))
    tokens = word_tokenize(review)
    final_tokens = [w for w in tokens if w not in stopword]
    finalwords=[]
    for w in final_tokens:
      if len(w)>1:
        word = wordLemm.lemmatize(w)
        finalwords.append(word)
    return ' '.join(finalwords)

df_all['processed_reviews'] = df_all['reviewText'].apply(lambda x: process_reviews(x))
print('Text Preprocessing complete.')

df_all

"""Vectorization and Splitting the data"""

X = df_all['processed_reviews'].values
y = df_all['overall'].values

print(X.shape)
print(y.shape)

"""Convert text to word frequency vectors"""

vector = TfidfVectorizer(sublinear_tf=True)
X = vector.fit_transform(X)
print(f'Vector fitted.')

print(X.shape)
print(y.shape)

"""Split train and test"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)

"""Evaluating the Model"""

def model_Evaluate(model):
    acc_train=model.score(X_train, y_train)
    acc_test=model.score(X_test, y_test)

    print('Accuracy of model on training data : {}'.format(acc_train*100))
    print('Accuracy of model on testing data : {} \n'.format(acc_test*100))

    y_pred = model.predict(X_test)

    print(classification_report(y_test, y_pred))

    cf_matrix = confusion_matrix(y_test, y_pred)

    print(cf_matrix)
    categories  = ['Negative','Positive']
    group_names = ['True Neg','False Pos', 'False Neg','True Pos']
    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]

    labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)

    sns.heatmap(cf_matrix, annot = labels, cmap = 'YlGn',fmt = '',
                xticklabels = categories, yticklabels = categories)

    plt.xlabel("Predicted values", fontdict = {'size':14}, labelpad = 10)
    plt.ylabel("Actual values"   , fontdict = {'size':14}, labelpad = 10)
    plt.title ("Confusion Matrix", fontdict = {'size':18}, pad = 20)

"""Logistic Regression"""

lg = LogisticRegression()
lg.fit(X_train, y_train)
model_Evaluate(lg)

"""Model Saving, Loading and Prediction"""

import pickle

file = open('vectoriser.pickle','wb')
pickle.dump(vector, file)
file.close()

file = open('logisticRegression.pickle','wb')
pickle.dump(lg, file)
file.close()

"""Conclusion

We can use the Logistic Regression for most cases and it will yeild good results.

Predict using saved model
"""

def load_models():
    file = open('vectoriser.pickle', 'rb')
    vectoriser = pickle.load(file)
    file.close()
    file = open('logisticRegression.pickle', 'rb')
    lg = pickle.load(file)
    file.close()
    return vectoriser, lg

def predict(vectoriser, model, text):
    processes_text=[process_reviews(sen) for sen in text]
    textdata = vectoriser.transform(processes_text)
    sentiment = model.predict(textdata)

    data = []
    for text, pred in zip(text, sentiment):
        data.append((text,pred))
    df = pd.DataFrame(data, columns = ['text','sentiment'])
    df = df.replace([0,1], ["Negative","Positive"])
    return df

if _name=="main_":
    vectoriser, lg = load_models()

    text = ["Such an amazing trimmer. It is a must buy for anyone who is thinking to buy a trimmer. It is a multipurpose trimmer which has got 10 modes with custom settings. Completely satisfied with the purchase.",
            "Absolute garbage. Do not buy this, I would give this zero star if possible.",
            "Definitely worth it if you can sell one of your kidney to afford it. The price makes it really unique from all other products."]

    df = predict(vectoriser, lg, text)
    print(df.head())